#!/usr/bin/env python3
"""Generate reference outputs for ALBERT integration tests.

Run with: uv run --with transformers --with torch scripts/generate_reference.py
"""

import torch
from transformers import AlbertForMaskedLM, AlbertTokenizer

MODEL_NAME = "albert/albert-base-v2"

# Test sentences
SENTENCES = [
    "The capital of France is [MASK].",
    "The [MASK] sat on the mat.",
    "She studied [MASK] at the university.",
]


def print_rust_const(name, values, fmt="f32"):
    """Print a Rust const array."""
    n = len(values)
    if fmt == "f32":
        items = ", ".join(f"{v:.10f}" for v in values)
        print(f"const {name}: [f32; {n}] = [{items}];")
    elif fmt == "i64":
        items = ", ".join(str(int(v)) for v in values)
        print(f"const {name}: [i64; {n}] = [{items}];")


def process_sentence(tokenizer, model, sentence, label):
    """Generate reference values for one sentence."""
    print(f"\n// === {label}: \"{sentence}\" ===")

    inputs = tokenizer(sentence, return_tensors="pt")
    input_ids = inputs["input_ids"]

    mask_token_id = tokenizer.mask_token_id
    mask_pos = (input_ids[0] == mask_token_id).nonzero(as_tuple=True)[0].item()

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits  # [1, seq_len, vocab_size]

    mask_logits = logits[0, mask_pos]  # [vocab_size]

    # First 10 logits
    first10 = mask_logits[:10].tolist()
    print_rust_const(f"{label}_FIRST_10_LOGITS", first10)

    # Top-5
    top5 = torch.topk(mask_logits, 5)
    print_rust_const(f"{label}_TOP5_IDS", top5.indices.tolist(), fmt="i64")
    print_rust_const(f"{label}_TOP5_LOGITS", top5.values.tolist())

    # Logit statistics: min, max, mean, sum (for full-vector sanity check)
    print(f"const {label}_LOGIT_MIN: f32 = {mask_logits.min().item():.10f};")
    print(f"const {label}_LOGIT_MAX: f32 = {mask_logits.max().item():.10f};")
    print(f"const {label}_LOGIT_MEAN: f32 = {mask_logits.mean().item():.10f};")

    # Full sequence logit norms (checks ALL positions, not just [MASK])
    seq_logits = logits[0]  # [seq_len, vocab_size]
    norms = seq_logits.norm(dim=-1).tolist()  # L2 norm per position
    print_rust_const(f"{label}_SEQ_NORMS", norms)

    # Print human-readable top-5
    print(f"// Top-5 predictions:")
    for i, (score, idx) in enumerate(zip(top5.values, top5.indices)):
        token = tokenizer.decode([idx.item()]).strip()
        print(f"//   {i+1}: \"{token}\" (logit: {score.item():.4f})")


def main():
    print(f"// Generated by scripts/generate_reference.py")
    print(f"// Model: {MODEL_NAME}")

    tokenizer = AlbertTokenizer.from_pretrained(MODEL_NAME)
    model = AlbertForMaskedLM.from_pretrained(MODEL_NAME)
    model.eval()

    labels = ["S1", "S2", "S3"]
    for sentence, label in zip(SENTENCES, labels):
        process_sentence(tokenizer, model, sentence, label)


if __name__ == "__main__":
    main()
