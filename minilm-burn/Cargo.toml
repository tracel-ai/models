[package]
authors = ["Dilshod Tadjibaev (@antimora)"]
license = "MIT OR Apache-2.0"
name = "minilm-burn"
version = "0.1.0"
edition = "2024"
description = "MiniLM sentence transformer with Burn"

[features]
default = ["pretrained"]
pretrained = ["burn/network", "dep:dirs"]

# Backend selection
ndarray = ["burn/ndarray"]
tch-cpu = ["burn/tch"]
tch-gpu = ["burn/tch"]
wgpu = ["burn/wgpu"]
cuda = ["burn/cuda"]

[dependencies]
burn = { version = "0.20.0", default-features = false, features = ["std"] }
burn-store = { version = "0.20.0", features = ["std", "safetensors"] }

# Tokenizer
tokenizers = { version = "0.19.1", default-features = false, features = ["onig"] }

# HuggingFace model download
hf-hub = { version = "0.4.3" }
dirs = { version = "6.0.0", optional = true }

# Serialization
serde = { version = "1.0", default-features = false, features = ["derive", "alloc"] }
serde_json = "1.0"

# Math
libm = "0.2"

# Async runtime for hf-hub
tokio = { version = "1.35", features = ["rt-multi-thread"] }

[dev-dependencies]
clap = { version = "4.5", features = ["derive"] }
criterion = "0.5"

[[example]]
name = "inference"
required-features = ["pretrained", "ndarray"]

[[bench]]
name = "inference"
harness = false
required-features = ["pretrained"]
